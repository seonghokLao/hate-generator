{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNx0qq6TfYmb"
      },
      "source": [
        "# LLM Based Red-Team Generator\n",
        "\n",
        "This notebook serves as a proof-of-concept for LLM based methods in generating diverse toxic, harmful and hateful content.\n",
        "\n",
        "To run tests, import this notebook to Google Colab, then upload `train.py`, `test.py`, `generate.py`, and `rtg-adapter.zip`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0K0oJvEgmdG"
      },
      "source": [
        "## 1. Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xUQ_w5OGZ0EE"
      },
      "outputs": [],
      "source": [
        "!pip install -U \"transformers>=4.45.0\" \"peft>=0.12.0\" \"trl>=0.9.0\" \"bitsandbytes>=0.43.0\" \"accelerate>=0.34.0\" \"datasets==3.6.0\" -q\n",
        "!pip install -U evaluate nltk absl-py rouge_score sacremoses sentence-transformers scipy -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uigdLLgvgTqa"
      },
      "source": [
        "## 2. Train model from scratch or load LoRA checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uXFan5hr6CEJ"
      },
      "outputs": [],
      "source": [
        "# !python colab_train.py\n",
        "!unzip \"rtg-adapter.zip\" -d \"rtg-adapter\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WADhhnEPgyv_"
      },
      "source": [
        "## 3. Configure desired model and scope of output\n",
        "You can choose between these models:\n",
        "*   `meta-llama/Llama-3.2-3B-Instruct`\n",
        "*   `dphn/Dolphin3.0-Llama3.2-3B`\n",
        "*   `dphn/Dolphin3.0-Llama3.2-3B + adapter (LoRA)`\n",
        "\n",
        "You can choose to generate toxic content targeting various groups of people."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEqMJXrNUICA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# --- 1. DEFINE YOUR CONFIGURATION HERE ---\n",
        "model_id = \"dphn/Dolphin3.0-Llama3.2-3B\"\n",
        "# model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "adapter_id = \"rtg-adapter\"\n",
        "# adapter_id = None\n",
        "\n",
        "# targets = [\"Women\"] * 10\n",
        "targets = [\n",
        "    \"Women\",\n",
        "    \"Muslims\",\n",
        "    \"Immigrants\",\n",
        "    \"Jewish people\",\n",
        "    \"African Americans\",\n",
        "    \"LGBTQ+ community\",\n",
        "    \"Liberals\",\n",
        "    \"Conservatives\",\n",
        "    \"The disabled\",\n",
        "    \"General\"\n",
        "]\n",
        "\n",
        "config_data = {\n",
        "    \"model_id\": model_id,\n",
        "    \"adapter_id\": adapter_id,\n",
        "    \"targets\": targets,\n",
        "    \"generation_params\": {\n",
        "        \"max_new_tokens\": 100,\n",
        "        \"do_sample\": True,\n",
        "        \"temperature\": 0.95, # Higher = more toxic/creative\n",
        "        \"top_p\": 0.90,\n",
        "        \"repetition_penalty\": 1.2,\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- 2. SAVE TO FILE ---\n",
        "config_path = \"model_config.json\"\n",
        "# config_path = \"baseline_config.json\"\n",
        "with open(config_path, \"w\") as f:\n",
        "    json.dump(config_data, f, indent=4)\n",
        "\n",
        "print(f\"Configuration saved to {config_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpdlXomPhkCp"
      },
      "source": [
        "## 4. Generate toxic content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UseVg88gnDTw"
      },
      "outputs": [],
      "source": [
        "# !python generate_baseline.py\n",
        "!python generate.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Uqqx8vxhy3m"
      },
      "source": [
        "## 5. Evaluate Model\n",
        "We evaluate our models based on toxicity and diversity.\n",
        "\n",
        "Toxicity is evaluated using `facebook/roberta-hate-speech-dynabench-r4-target`.\n",
        "\n",
        "Diversity is evaluated using Self-BLEU metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoAj_xj6aZeV"
      },
      "outputs": [],
      "source": [
        "!python test.py --file model_outputs.csv"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
